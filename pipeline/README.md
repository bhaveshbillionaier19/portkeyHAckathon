# ğŸš€ Automated Weekly Evaluation Pipeline

## Quick Start

### Run the Pipeline

```bash
# Test run (dry run mode)
python pipeline/master.py

# Full evaluation
# Edit pipeline/config.yaml and set dry_run: false
python pipeline/master.py
```

### Configuration

Edit `pipeline/config.yaml` to customize:
- **Models to evaluate**: Update `portkey_id` values
- **Questions per category**: Adjust `questions_per_category`
- **Schedule**: Set `day`, `hour`, `minute`
- **Notifications**: Enable email/Slack

### Directory Structure

```
pipeline/
â”œâ”€â”€ master.py          # Main orchestrator
â”œâ”€â”€ config.yaml        # Configuration
â””â”€â”€ scheduler/         # Scheduling scripts

logs/weekly/           # Evaluation logs
â””â”€â”€ YYYY-MM-DD/
    â”œâ”€â”€ pipeline.log
    â”œâ”€â”€ test_gemini.log
    â”œâ”€â”€ test_gpt.log
    â”œâ”€â”€ test_claude.log
    â”œâ”€â”€ debate_results.json
    â”œâ”€â”€ rankings.json
    â””â”€â”€ report.txt

archive/evaluations/   # Archived results
```

### Features

âœ… **Automated Evaluation**: Runs all models automatically  
âœ… **Debate Mechanism**: Multi-judge scoring  
âœ… **Auto-Routing**: Updates to best models  
âœ… **Version Tracking**: Uses latest model versions  
âœ… **Notifications**: Email/Slack alerts  
âœ… **Archival**: Keeps last 12 weeks of results  

### Development

1. **Test Mode**: Set `dry_run: true` in config
2. **Run Once**: `python pipeline/master.py`
3. **Schedule**: See `pipeline/scheduler/` for setup

### Next Steps

1. âœ… Basic pipeline working
2. â³ Implement version checker
3. â³ Connect to actual evaluation scripts
4. â³ Set up weekly scheduler

See `pipeline_plan.md` for full roadmap!
